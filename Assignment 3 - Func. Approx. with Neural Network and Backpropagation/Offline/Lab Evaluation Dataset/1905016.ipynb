{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "with open('a1.pkl', 'rb') as a1:\n",
    "  test_dataset = pickle.load(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def conv_to_dataframe(dataset):\n",
    "    x = []\n",
    "    y = []\n",
    "    for img,label in dataset:\n",
    "        x.append(np.array(img).flatten())\n",
    "        y.append(label)\n",
    "    dataframe = pd.DataFrame(x)\n",
    "    dataframe['target'] = y\n",
    "    return dataframe\n",
    "\n",
    "def preprocessing(dataframe,target):\n",
    "    # drop target column null datapoints\n",
    "    dataframe.dropna(subset=[target], inplace=True)\n",
    "\n",
    "    # fill in missing null values\n",
    "    for i in dataframe:\n",
    "        if dataframe[i].dtypes=='object':\n",
    "            dataframe.fillna({i:dataframe[i].mode()[0]}, inplace=True)\n",
    "        else:\n",
    "            dataframe.fillna({i:dataframe[i].mean()}, inplace=True)\n",
    "    \n",
    "    # drop duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    \n",
    "    Features = dataframe.drop(target,axis=1)\n",
    "    Labels = dataframe[target]\n",
    "    Labels = Labels.to_numpy().astype(int)\n",
    "\n",
    "    one_hot = np.zeros((Labels.shape[0],10))\n",
    "    one_hot[np.arange(Labels.shape[0]),Labels] = 1\n",
    "\n",
    "    Features = Features.to_numpy().astype(float)\n",
    "    \n",
    "    # For scaling matching with train dataset\n",
    "    Features = (Features - np.min(Features)) / (np.max(Features) - np.min(Features))\n",
    "\n",
    "    # one_hot = one_hot.to_numpy().astype(float)\n",
    "    return Features, one_hot\n",
    "\n",
    "# train_dataframe = conv_to_dataframe(train_dataset)\n",
    "test_dataframe = conv_to_dataframe(test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,y_train = preprocessing(train_dataframe,'target')\n",
    "x_test,y_test = preprocessing(test_dataframe,'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN and other necessary classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score,recall_score,confusion_matrix,precision_score,f1_score,roc_auc_score,average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0,self.input)\n",
    "    \n",
    "    def backward(self, input):\n",
    "        temp = input.copy()\n",
    "        temp[self.input<0] = 0\n",
    "        return temp\n",
    "    \n",
    "class Dense:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        temp = np.sqrt(6 / (n_input + n_output))\n",
    "        self.weight = np.random.uniform(-temp,temp,(n_input, n_output))\n",
    "        self.bias = np.random.uniform(-temp,temp,(1, n_output))\n",
    "        self.input = self.bias_grad = self.weight_grad = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return (self.input @ self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, input):\n",
    "        self.bias_grad = np.sum(input,axis=0)\n",
    "        self.weight_grad = self.input.T @ input\n",
    "        return input @ self.weight.T\n",
    "    \n",
    "class BatchNormalization:\n",
    "    def __init__(self,input_n,momentum=0.9):\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, input_n))\n",
    "        self.beta = np.zeros((1, input_n))\n",
    "        self.running_var = np.ones((1, input_n))\n",
    "        self.running_mean = np.zeros((1, input_n))\n",
    "        self.x = self.beta_grad = self.gamma_grad = None\n",
    "\n",
    "    def forward(self,input,is_training):\n",
    "        epsilon = 1e-12\n",
    "        if is_training:\n",
    "            self.x = input\n",
    "            mean = np.mean(self.x,axis=0)\n",
    "            var = np.var(self.x,axis=0)\n",
    "            self.running_mean = self.running_mean*self.momentum + (1-self.momentum)*mean\n",
    "            self.running_var = self.running_var*self.momentum + (1-self.momentum)*var\n",
    "            return self.gamma*(self.x-mean)/np.sqrt(var+epsilon) + self.beta\n",
    "        else:\n",
    "            return self.gamma*(input-self.running_mean)/np.sqrt(self.running_var+epsilon) + self.beta\n",
    "        \n",
    "    def backward(self,input):\n",
    "        epsilon = 1e-12\n",
    "        mean = np.mean(self.x,axis=0)\n",
    "        var = np.var(self.x,axis=0)\n",
    "        self.beta_grad = np.sum(input, axis=0)\n",
    "        self.gamma_grad = np.sum(input*(self.x-mean)/np.sqrt(var+epsilon), axis=0)\n",
    "        d_var = -0.5 * self.gamma * np.power(var+epsilon,-3/2) * np.sum((self.x-mean)*input,axis=0)\n",
    "        d_mean = (-self.gamma/np.sqrt(var+epsilon))*np.sum(input,axis=0) + d_var * -2 * np.sum(self.x-mean,axis=0)/input.shape[0]\n",
    "        dx = input * self.gamma/np.sqrt(var+epsilon) + d_var * 2 * (self.x-mean)/input.shape[0] + d_mean/input.shape[0]\n",
    "        return dx\n",
    "    \n",
    "class Dropout:\n",
    "    def __init__(self,rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, input, is_training):\n",
    "        if is_training:\n",
    "            self.mask = (np.random.rand(1,input.shape[1])>self.rate).astype(np.float32)\n",
    "            return self.mask*input/(1-self.rate)\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, input):\n",
    "        return self.mask*input/(1-self.rate)\n",
    "\n",
    "class SoftMax:\n",
    "    def __init__(self):\n",
    "        self.input = self.output = self.r_input = self.r_output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        temp = self.input-np.max(self.input,axis=1).reshape(-1,1)\n",
    "        temp = np.exp(temp)\n",
    "        self.output = temp/np.sum(temp,axis=1).reshape(-1,1)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self,input):\n",
    "        self.r_input = input\n",
    "        self.r_output = np.empty((0,self.r_input.shape[1]))\n",
    "        for i in range(len(self.r_input)):\n",
    "            temp = np.zeros((self.r_input.shape[1],self.r_input.shape[1]))\n",
    "            for j in range(temp.shape[1]):\n",
    "                for k in range(temp.shape[1]):\n",
    "                    if (j==k):\n",
    "                        temp[k,j] = self.output[i,j]*(1-self.output[i,j])\n",
    "                    else:\n",
    "                        temp[k,j] = -self.output[i,j]*self.output[i,k]\n",
    "            temp = self.r_input[i] @ temp\n",
    "            self.r_output = np.vstack([self.r_output,temp])\n",
    "        return self.r_output\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self,y_pred,y):\n",
    "        temp = np.log(y_pred)\n",
    "        temp = -temp*y\n",
    "        temp = np.sum(temp,axis=1)\n",
    "        return np.mean(temp)\n",
    "    \n",
    "    def backward(self,y_pred,y):\n",
    "        temp = - y/y_pred\n",
    "        return temp/y.shape[0]\n",
    "    \n",
    "class SquaredErrorLoss:\n",
    "    def forward(self,y_pred,y):\n",
    "        return np.mean(np.sum(np.power(y-y_pred,2),axis=1))\n",
    "\n",
    "\n",
    "    def backward(self,y_pred,y):\n",
    "        return (-2*(y-y_pred))/y.shape[0]\n",
    "\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.optimizer = None\n",
    "        self.loss = None\n",
    "        self.metrics = ['loss']\n",
    "    \n",
    "    def add_layer(self,layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self,optimizer,loss,metrics=[]):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        if hasattr(self.optimizer,'initialize_params'):\n",
    "            self.optimizer.initialize_params(self.layers)\n",
    "        self.metrics = ['loss'] + metrics\n",
    "\n",
    "    def performance_metrics(self,y, y_pred, y_pred_prob):\n",
    "        result = {}\n",
    "        if ('accuracy' in self.metrics):\n",
    "            result['accuracy'] = accuracy_score(y, y_pred)\n",
    "        if ('recall' in self.metrics):\n",
    "            result['recall'] = recall_score(y, y_pred, average='weighted', zero_division=0)\n",
    "        if ('precision' in self.metrics):\n",
    "            result['precision'] = precision_score(y, y_pred, average='weighted', zero_division=0)\n",
    "        if ('f1-score' in self.metrics):\n",
    "            result['f1-score'] = f1_score(y, y_pred, average='macro', zero_division=0)\n",
    "        if ('auroc' in self.metrics):\n",
    "            result['auroc'] = roc_auc_score(y, y_pred_prob, average='weighted', multi_class='ovr')\n",
    "        if ('aupr' in self.metrics):\n",
    "            result['aupr'] = average_precision_score(y, y_pred_prob, average='weighted')\n",
    "        return result\n",
    "    \n",
    "    def save_model(self,name):\n",
    "        for i in self.layers:\n",
    "            if isinstance(i,Dense):\n",
    "                i.input = i.bias_grad = i.weight_grad = None\n",
    "            elif isinstance(i,ReLU):\n",
    "                i.input = None\n",
    "            elif isinstance(i,BatchNormalization):\n",
    "                i.x = i.beta_grad = i.gamma_grad = None\n",
    "            elif isinstance(i,Dropout):\n",
    "                i.mask = None\n",
    "            elif isinstance(i,SoftMax):\n",
    "                i.input = i.output = i.r_input = i.r_output = None\n",
    "        with open(name, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "\n",
    "\n",
    "    def forward(self,input,is_training):\n",
    "        temp = input.copy()\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer,Dropout) or isinstance(layer,BatchNormalization):\n",
    "                temp = layer.forward(temp,is_training)\n",
    "            else:\n",
    "                temp = layer.forward(temp)\n",
    "        return temp\n",
    "    \n",
    "    def backward(self,input):\n",
    "        temp = input.copy()\n",
    "        for layer in reversed(self.layers):\n",
    "            temp = layer.backward(temp)\n",
    "        return temp\n",
    "    \n",
    "    def evaluate(self,X,y):\n",
    "        y_pred = self.forward(X,False)\n",
    "        epsilon = 1e-12\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        loss = self.loss.forward(y_pred,y) \n",
    "\n",
    "        one_hot = np.zeros_like(y_pred)  \n",
    "        max_indices = np.argmax(y_pred, axis=1)  \n",
    "        one_hot[np.arange(y_pred.shape[0]), max_indices] = 1\n",
    "\n",
    "        res = self.performance_metrics(y,one_hot,y_pred)\n",
    "        res = {'loss': loss, **res}\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.forward(X,False)\n",
    "    \n",
    "    def train(self,X,y,epochs,batch_size=64,validation_split=0,show_graph=False):\n",
    "        ind = np.arange(y.shape[0])\n",
    "        np.random.shuffle(ind)\n",
    "        train_indices = ind[:int(y.shape[0] * (1 - validation_split))]\n",
    "        val_indices = ind[int(y.shape[0] * (1 - validation_split)):]\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        batch_n = int(np.ceil(y_train.shape[0]/batch_size))\n",
    "        training_metric = []\n",
    "        validation_metric = []\n",
    "        confusion_matrix_training = None\n",
    "        confusion_matrix_validation = None\n",
    "        res = None\n",
    "        v_res = None\n",
    "        for i in range(epochs):\n",
    "            print(f\"Epoch {i+1}/{epochs}\")\n",
    "            loss = 0\n",
    "            total_y_pred_prob = np.empty((0,y_train.shape[1]))\n",
    "            total_y_pred = np.empty((0,y_train.shape[1]))\n",
    "            total_y = np.empty((0,y_train.shape[1]))\n",
    "            indices = np.arange(y_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            pbar =tqdm(total=batch_n)\n",
    "            for j in range(batch_n):\n",
    "                batch_indices = indices[j * batch_size:(j + 1) * batch_size]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "\n",
    "                y_pred = self.forward(X_batch,True)\n",
    "                epsilon = 1e-12\n",
    "                y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "                loss += self.loss.forward(y_pred,y_batch) \n",
    "\n",
    "                temp = self.loss.backward(y_pred,y_batch)\n",
    "                self.backward(temp)\n",
    "\n",
    "                self.optimizer.update(self.layers)\n",
    "                \n",
    "                one_hot = np.zeros_like(y_pred)  \n",
    "                max_indices = np.argmax(y_pred, axis=1)  \n",
    "                one_hot[np.arange(y_pred.shape[0]), max_indices] = 1\n",
    "\n",
    "                total_y_pred_prob = np.vstack([total_y_pred_prob,y_pred])\n",
    "                total_y_pred = np.vstack([total_y_pred,one_hot])\n",
    "                total_y = np.vstack([total_y,y_batch])\n",
    "\n",
    "                res = self.performance_metrics(total_y,total_y_pred,total_y_pred_prob)\n",
    "                res = {'loss': loss/(j+1), **res}\n",
    "\n",
    "                # print(f\"{', '.join(f\"{key}: {res[key]}\" for key in res)}\")\n",
    "                pbar.set_postfix(res)\n",
    "                pbar.update(1)\n",
    "\n",
    "            self.optimizer.update_learning_rate(learning_rate_scheduler(self.optimizer.learning_rate,i+1))\n",
    "            confusion_matrix_training = confusion_matrix(np.argmax(total_y,axis=1),np.argmax(total_y_pred,axis=1))\n",
    "            res = {'epoch': i+1, **res}\n",
    "            training_metric.append(res)\n",
    "            if validation_split>0:\n",
    "                v_res = self.evaluate(X_val,y_val)\n",
    "                print(f\"{', '.join(f\"validation {key}: {v_res[key]}\" for key in v_res)}\")\n",
    "                y_pred = self.predict(X_val)\n",
    "                one_hot = np.zeros_like(y_pred)  \n",
    "                max_indices = np.argmax(y_pred, axis=1)  \n",
    "                one_hot[np.arange(y_pred.shape[0]), max_indices] = 1\n",
    "                confusion_matrix_validation = confusion_matrix(np.argmax(y_val,axis=1),np.argmax(one_hot,axis=1))\n",
    "            v_res = {'epoch': i+1, **v_res}\n",
    "            validation_metric.append(v_res)\n",
    "            pbar.close()\n",
    "        \n",
    "        # print(f\"{', '.join(f\"training {key}: {res[key]}\" for key in res)}\")\n",
    "        # print(f\"{', '.join(f\"validation {key}: {v_res[key]}\" for key in v_res)}\")\n",
    "\n",
    "        if show_graph:\n",
    "            fields = {key: [metric[key] for metric in training_metric] for key in training_metric[0]}\n",
    "            for field, values in fields.items():\n",
    "                if field != 'epoch':\n",
    "                    plt.plot(fields['epoch'], values, label=\"training \" + field)\n",
    "            if validation_split>0:\n",
    "                fields = {key: [metric[key] for metric in validation_metric] for key in validation_metric[0]}\n",
    "                for field, values in fields.items():\n",
    "                    if field != 'epoch':\n",
    "                        plt.plot(fields['epoch'], values, label=\"validation \" + field)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Metric Value')\n",
    "            plt.title('Metrics Over Epochs')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(\"Training confusion matrix:\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(confusion_matrix_training, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "            plt.xlabel(\"Predicted Label\")\n",
    "            plt.ylabel(\"True Label\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.show()\n",
    "            print(\"Validation confusion matrix:\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(confusion_matrix_validation, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "            plt.xlabel(\"Predicted Label\")\n",
    "            plt.ylabel(\"True Label\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.show()\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self,learning_rate = 0.01,beta1=0.9,beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_learning_rate = self.learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.t = 1\n",
    "        self.m = []\n",
    "        self.v = []\n",
    "\n",
    "    def initialize_params(self,layers):\n",
    "        for i in layers:\n",
    "            if isinstance(i,Dense):\n",
    "                self.m.append([np.zeros_like(i.bias),np.zeros_like(i.weight)])\n",
    "                self.v.append([np.zeros_like(i.bias),np.zeros_like(i.weight)])\n",
    "            elif isinstance(i,BatchNormalization):\n",
    "                self.m.append([np.zeros_like(i.gamma),np.zeros_like(i.beta)])\n",
    "                self.v.append([np.zeros_like(i.gamma),np.zeros_like(i.beta)])\n",
    "\n",
    "    def update_learning_rate(self,learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self,layers):\n",
    "        epsilon = 1e-12\n",
    "        j = 0\n",
    "        for i in layers:\n",
    "            if isinstance(i,Dense):\n",
    "                self.m[j][0] = self.m[j][0]*self.beta1+(1-self.beta1)*i.bias_grad\n",
    "                self.m[j][1] = self.m[j][1]*self.beta1+(1-self.beta1)*i.weight_grad\n",
    "                self.v[j][0] = self.v[j][0]*self.beta2+(1-self.beta2)*np.power(i.bias_grad,2)\n",
    "                self.v[j][1] = self.v[j][1]*self.beta2+(1-self.beta2)*np.power(i.weight_grad,2)\n",
    "\n",
    "                temp1 = self.m[j][0]/(1-(self.beta1**self.t))\n",
    "                temp2 = self.v[j][0]/(1-(self.beta2**self.t))\n",
    "\n",
    "                i.bias -= (self.learning_rate*temp1)/(np.sqrt(temp2)+epsilon)\n",
    "\n",
    "                temp1 = self.m[j][1]/(1-(self.beta1**self.t))\n",
    "                temp2 = self.v[j][1]/(1-(self.beta2**self.t))\n",
    "\n",
    "                i.weight -= (self.learning_rate*temp1)/(np.sqrt(temp2)+epsilon)\n",
    "                j += 1\n",
    "\n",
    "            elif isinstance(i,BatchNormalization):\n",
    "                self.m[j][0] = self.m[j][0]*self.beta1+(1-self.beta1)*i.gamma_grad\n",
    "                self.m[j][1] = self.m[j][1]*self.beta1+(1-self.beta1)*i.beta_grad\n",
    "                self.v[j][0] = self.v[j][0]*self.beta2+(1-self.beta2)*np.power(i.gamma_grad,2)\n",
    "                self.v[j][1] = self.v[j][1]*self.beta2+(1-self.beta2)*np.power(i.beta_grad,2)\n",
    "\n",
    "                temp1 = self.m[j][0]/(1-(self.beta1**self.t))\n",
    "                temp2 = self.v[j][0]/(1-(self.beta2**self.t))\n",
    "\n",
    "                i.gamma -= (self.learning_rate*temp1)/(np.sqrt(temp2)+epsilon)\n",
    "\n",
    "                temp1 = self.m[j][1]/(1-(self.beta1**self.t))\n",
    "                temp2 = self.v[j][1]/(1-(self.beta2**self.t))\n",
    "\n",
    "                i.beta -= (self.learning_rate*temp1)/(np.sqrt(temp2)+epsilon)\n",
    "                j += 1\n",
    "\n",
    "        self.t += 1\n",
    "    \n",
    "def learning_rate_scheduler(learning_rate,epoch):\n",
    "    if (epoch%10==0):\n",
    "        learning_rate = learning_rate*0.8\n",
    "    return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fnn = FNN()\n",
    "# fnn.add_layer(Dense(784,512))\n",
    "# fnn.add_layer(BatchNormalization(512))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.4))\n",
    "# fnn.add_layer(Dense(512,256))\n",
    "# fnn.add_layer(BatchNormalization(256))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.3))\n",
    "# fnn.add_layer(Dense(256,64))\n",
    "# fnn.add_layer(BatchNormalization(64))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.2))\n",
    "# fnn.add_layer(Dense(64,10))\n",
    "# fnn.add_layer(BatchNormalization(10))\n",
    "# fnn.add_layer(SoftMax())\n",
    "\n",
    "# fnn.compile(optimizer=Adam(learning_rate=0.01),loss=CrossEntropyLoss(),metrics=['accuracy','f1-score'])\n",
    "# fnn.train(x_train,y_train,epochs=100,batch_size=10000,validation_split=0.2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation for 4 diffferent learning rate and 3 different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# learning_rate = [0.005,0.003,0.001,0.01]\n",
    "# model = []\n",
    "\n",
    "# # model 1\n",
    "# fnn = FNN()\n",
    "# fnn.add_layer(Dense(784,256))\n",
    "# fnn.add_layer(BatchNormalization(256))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.5))\n",
    "# fnn.add_layer(Dense(256,64))\n",
    "# fnn.add_layer(BatchNormalization(64))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.5))\n",
    "# fnn.add_layer(Dense(64,10))\n",
    "# fnn.add_layer(BatchNormalization(10))\n",
    "# fnn.add_layer(SoftMax())\n",
    "\n",
    "# model.append(fnn)\n",
    "\n",
    "# # model 2\n",
    "# fnn = FNN()\n",
    "# fnn.add_layer(Dense(784,512))\n",
    "# fnn.add_layer(BatchNormalization(512))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.4))\n",
    "# fnn.add_layer(Dense(512,256))\n",
    "# fnn.add_layer(BatchNormalization(256))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.3))\n",
    "# fnn.add_layer(Dense(256,64))\n",
    "# fnn.add_layer(BatchNormalization(64))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.2))\n",
    "# fnn.add_layer(Dense(64,10))\n",
    "# fnn.add_layer(BatchNormalization(10))\n",
    "# fnn.add_layer(SoftMax())\n",
    "\n",
    "# model.append(fnn)\n",
    "\n",
    "# # model 3\n",
    "# fnn = FNN()\n",
    "# fnn.add_layer(Dense(784,128))\n",
    "# fnn.add_layer(BatchNormalization(128))\n",
    "# fnn.add_layer(ReLU())\n",
    "# fnn.add_layer(Dropout(0.5))\n",
    "# fnn.add_layer(Dense(128,10))\n",
    "# fnn.add_layer(BatchNormalization(10))\n",
    "# fnn.add_layer(SoftMax())\n",
    "\n",
    "# model.append(fnn)\n",
    "\n",
    "# model = [copy.deepcopy(model) for i in range(len(learning_rate))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(learning_rate)):\n",
    "#     j = 0\n",
    "#     for fnn in model[i]:\n",
    "#         print(f\"Model: {j+1}, Learning rate: {learning_rate[i]}\")\n",
    "#         fnn.compile(optimizer=Adam(learning_rate=learning_rate[i]),loss=CrossEntropyLoss(),metrics=['accuracy','f1-score'])\n",
    "#         fnn.train(x_train,y_train,epochs=50,batch_size=10000,validation_split=0.2,show_graph=True)\n",
    "#         j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model[3][2].save_model('model_1905016.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the test set on best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.979001640507303, test accuracy: 0.6970368319025201, test f1-score: 0.30829199830090637\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('model_1905016.pickle', 'rb') as file:\n",
    "    fnn = pickle.load(file)\n",
    "\n",
    "\n",
    "res = fnn.evaluate(x_test,y_test)\n",
    "print(f\"{', '.join(f\"test {key}: {res[key]}\" for key in res)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
