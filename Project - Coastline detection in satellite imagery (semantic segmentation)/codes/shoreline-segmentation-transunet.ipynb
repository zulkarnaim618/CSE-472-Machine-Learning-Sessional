{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:04.991832Z",
     "iopub.status.busy": "2024-12-16T06:22:04.990835Z",
     "iopub.status.idle": "2024-12-16T06:22:08.428578Z",
     "shell.execute_reply": "2024-12-16T06:22:08.427636Z",
     "shell.execute_reply.started": "2024-12-16T06:22:04.991779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:08.430824Z",
     "iopub.status.busy": "2024-12-16T06:22:08.430338Z",
     "iopub.status.idle": "2024-12-16T06:22:08.439729Z",
     "shell.execute_reply": "2024-12-16T06:22:08.439062Z",
     "shell.execute_reply.started": "2024-12-16T06:22:08.430784Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_RANDOM_SEED = 2021\n",
    "\n",
    "seed = DEFAULT_RANDOM_SEED\n",
    "\n",
    "random.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:08.440807Z",
     "iopub.status.busy": "2024-12-16T06:22:08.440576Z",
     "iopub.status.idle": "2024-12-16T06:22:08.458322Z",
     "shell.execute_reply": "2024-12-16T06:22:08.457543Z",
     "shell.execute_reply.started": "2024-12-16T06:22:08.440785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''DATA LOADER'''\n",
    "class SWEDDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None, target_transform=None, index=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.index = index\n",
    "\n",
    "        self.data_dir = os.path.join(root_dir, mode)\n",
    "        self.image_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.label_dir = os.path.join(self.data_dir, 'labels')\n",
    "        \n",
    "        image_files = sorted([f for f in os.listdir(self.image_dir) \n",
    "                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n",
    "        label_files = sorted([f for f in os.listdir(self.label_dir) \n",
    "                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n",
    "        \n",
    "        self.pairs = []\n",
    "        label_suffix = '_chip_' if mode in ['train', 'val'] else '_label_'\n",
    "        image_dict = {f.replace('_image_', label_suffix): f for f in image_files}\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            if label_file in image_dict:\n",
    "                self.pairs.append((image_dict[label_file], label_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file, label_file = self.pairs[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        \n",
    "        if self.mode in ['train', 'val']:\n",
    "            image = np.load(img_path)\n",
    "            label = np.load(label_path)\n",
    "        else:\n",
    "            image = tifffile.imread(img_path)\n",
    "            label = tifffile.imread(label_path)\n",
    "            \n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.mode in ['train', 'val']:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        elif self.mode == 'test':\n",
    "            image = image.permute(0, 2, 1)\n",
    "            label = label.unsqueeze(0)\n",
    "            label = torch.rot90(label, 1, [1, 2])\n",
    "            label = torch.flip(label, [1])\n",
    "\n",
    "        image = image / 2.0**15     # jp2 images are 8 to 16 bit\n",
    "        label = label > 0.0         # binary label\n",
    "\n",
    "        # bands: B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B11, B12\n",
    "        if self.index:\n",
    "            ndwi1 = (image[2] - image[7]) / (image[2] + image[7] + 1e-6)\n",
    "            \n",
    "            # downsample\n",
    "            lowres = image[2].T.cpu().numpy()\n",
    "            original_shape = lowres.shape\n",
    "            lowres = cv2.resize(\n",
    "                lowres, \n",
    "                (lowres.shape[1] // 2, lowres.shape[0] // 2), \n",
    "                interpolation=cv2.INTER_CUBIC\n",
    "            )\n",
    "            lowres = cv2.resize(\n",
    "                lowres, \n",
    "                (original_shape[1], original_shape[0]), \n",
    "                interpolation=cv2.INTER_CUBIC\n",
    "            )\n",
    "            lowres = torch.from_numpy(lowres.T).float()\n",
    "\n",
    "            ndwi2 = (lowres - image[10]) / (lowres + image[10] + 1e-6)\n",
    "\n",
    "            image = torch.cat([image, ndwi1.unsqueeze(0), ndwi2.unsqueeze(0)], dim=0)\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:08.459528Z",
     "iopub.status.busy": "2024-12-16T06:22:08.459266Z",
     "iopub.status.idle": "2024-12-16T06:22:08.473117Z",
     "shell.execute_reply": "2024-12-16T06:22:08.472297Z",
     "shell.execute_reply.started": "2024-12-16T06:22:08.459504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''GET DATA LOADER'''\n",
    "def get_dataloaders(root_dir, batch_size=32, num_workers=4, train_transform=None, test_transform=None, device='cuda'):\n",
    "    train_dataset = SWEDDataset(root_dir, mode='train', transform=train_transform)\n",
    "    test_dataset = SWEDDataset(root_dir, mode='test', transform=test_transform)\n",
    "\n",
    "    # train_dataset, val_dataset, _ = random_split(train_dataset, [round(0.04 * len(train_dataset)), \n",
    "    #                                                              round(0.02 * len(train_dataset)), \n",
    "    #                                                              len(train_dataset) - round(0.06 * len(train_dataset))])\n",
    "\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [int(0.8 * len(train_dataset)),  \n",
    "                                                                 len(train_dataset) - int(0.8 * len(train_dataset))])\n",
    "\n",
    "    print(f'Train size: {len(train_dataset)}')\n",
    "    print(f'Validation size: {len(val_dataset)}')\n",
    "    print(f'Test size: {len(test_dataset)}')\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:08.475421Z",
     "iopub.status.busy": "2024-12-16T06:22:08.475162Z",
     "iopub.status.idle": "2024-12-16T06:22:08.487744Z",
     "shell.execute_reply": "2024-12-16T06:22:08.486889Z",
     "shell.execute_reply.started": "2024-12-16T06:22:08.475396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''DISPLAY SAMPLES'''\n",
    "def display_samples(dataloader, num_samples=5, index=True):\n",
    "    # Get a batch\n",
    "    images, masks = next(iter(dataloader))\n",
    "\n",
    "    # Move to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "\n",
    "    # Only display up to the requested number of samples\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    rows = 4 if index else 2\n",
    "    fig, axes = plt.subplots(rows, num_samples, figsize=(5*num_samples, 5*rows))\n",
    "    \n",
    "    idx = 0\n",
    "    while(idx < num_samples):\n",
    "        i = np.random.randint(0, len(images))\n",
    "        if masks[i].sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # Display RGB channels (assuming bands 3,2,1 are RGB)\n",
    "        rgb_img = images[i][[3,2,1]].permute(1,2,0)\n",
    "        # Normalize for visualization\n",
    "        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "        \n",
    "        axes[0, idx].imshow(rgb_img)\n",
    "        axes[0, idx].axis('off')\n",
    "        axes[0, idx].set_title(f'Image {idx+1}')\n",
    "        \n",
    "        axes[1, idx].imshow(masks[i][0], cmap='gray')\n",
    "        axes[1, idx].axis('off')\n",
    "        axes[1, idx].set_title(f'Mask {idx+1}')\n",
    "\n",
    "        if index:\n",
    "            axes[2, idx].imshow(images[i][12], cmap='gray')\n",
    "            axes[2, idx].axis('off')\n",
    "            axes[2, idx].set_title(f'NDWI1 {idx+1}')\n",
    "\n",
    "            axes[3, idx].imshow(images[i][13], cmap='gray')\n",
    "            axes[3, idx].axis('off')\n",
    "            axes[3, idx].set_title(f'NDWI2 {idx+1}')\n",
    "\n",
    "        idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:08.489070Z",
     "iopub.status.busy": "2024-12-16T06:22:08.488775Z",
     "iopub.status.idle": "2024-12-16T06:22:09.581760Z",
     "shell.execute_reply": "2024-12-16T06:22:09.580951Z",
     "shell.execute_reply.started": "2024-12-16T06:22:08.489045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''VISUALIZE SAMPLES'''\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "root_dir = \"/kaggle/input/sentinel-2-water-edges-dataset/SWED\"\n",
    "train_loader, val_loader, test_loader = get_dataloaders(root_dir, batch_size=16, num_workers=4)\n",
    "\n",
    "# Display 5 samples from training set\n",
    "# display_samples(train_loader, num_samples=5)\n",
    "# display_samples(test_loader, num_samples=5)\n",
    "\n",
    "'''\n",
    "# in case we need standardization\n",
    "\n",
    "channel-wise mean:  tensor([ 532.5187,  636.4246,  892.5240, 1049.9366, 1307.1577, 1738.9155,\n",
    "        1915.7476, 1995.0083, 2055.7939, 2086.2705, 2001.6875, 1491.3577])\n",
    "channel-wise std:  tensor([ 679.3956,  750.0253,  923.6580, 1273.5732, 1366.0400, 1500.5621,\n",
    "        1623.3806, 1687.1169, 1720.2144, 1827.5625, 1932.8875, 1631.7715])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:09.583423Z",
     "iopub.status.busy": "2024-12-16T06:22:09.583058Z",
     "iopub.status.idle": "2024-12-16T06:22:20.351290Z",
     "shell.execute_reply": "2024-12-16T06:22:20.350125Z",
     "shell.execute_reply.started": "2024-12-16T06:22:09.583386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:20.353662Z",
     "iopub.status.busy": "2024-12-16T06:22:20.352987Z",
     "iopub.status.idle": "2024-12-16T06:22:22.733581Z",
     "shell.execute_reply": "2024-12-16T06:22:22.732645Z",
     "shell.execute_reply.started": "2024-12-16T06:22:20.353617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''TRAINER CLASS'''\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryJaccardIndex, \n",
    "    BinaryCohenKappa, BinaryMatthewsCorrCoef, Accuracy\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader, device, \n",
    "                 scheduler=None, early_stopping_patience=10, min_delta=0.001):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.predictions = None\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "    def save_checkpoint(self, epoch, train_loss, val_loss, best_model=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        if self.scheduler:\n",
    "            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()\n",
    "\n",
    "        if best_model:\n",
    "            save_path = 'best_model.pt'\n",
    "        else:\n",
    "            save_path = 'checkpoint.pt'\n",
    "        torch.save(checkpoint, save_path)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path=\"best_model.pt\"):\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            return 0  # Start from scratch if no checkpoint exists\n",
    "            \n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if self.scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Restore loss history\n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_losses = checkpoint.get('val_losses', [])\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(self.train_loader, desc=\"train\"):\n",
    "            images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        return running_loss / len(self.train_loader.dataset)\n",
    "\n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"validation\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        return running_loss / len(self.val_loader.dataset)\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, num_epochs, do_plot=True, plot_interval=5, resume=False):\n",
    "        # Initialize or restore from checkpoint\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        start_epoch = self.load_checkpoint('checkpoint.pt') if resume else 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "        if start_epoch > 0:\n",
    "            print(\"Training resumed from epoch \", start_epoch)\n",
    "    \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            if self.early_stopped:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.val_epoch()\n",
    "    \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "    \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            self.save_checkpoint(epoch + 1, train_loss, val_loss)\n",
    "    \n",
    "            # Early stopping logic\n",
    "            if val_loss < self.best_val_loss - self.min_delta:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.early_stopping_counter = 0\n",
    "                # Save the best model\n",
    "                self.save_checkpoint(epoch + 1, train_loss, val_loss, best_model=True)\n",
    "                print(f\"New best model saved at epoch {epoch + 1}\")\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "                print(f\"No improvement. Early stopping counter: {self.early_stopping_counter}\")\n",
    "                \n",
    "                if self.early_stopping_counter >= self.early_stopping_patience:\n",
    "                    self.early_stopped = True\n",
    "                    print(\"Early stopping triggered.\")\n",
    "    \n",
    "            if do_plot and (epoch > 0 and epoch % plot_interval == 0 or epoch == num_epochs - 1):\n",
    "                self.plot_losses(self.train_losses, self.val_losses)\n",
    "        \n",
    "        # Load the best model at the end of training\n",
    "        if os.path.exists('best_model.pt'):\n",
    "            self.load_checkpoint('best_model.pt')\n",
    "        \n",
    "        return self.early_stopped\n",
    "\n",
    "\n",
    "    def test(self, dataset, thres=0.5):\n",
    "        self.load_checkpoint('best_model.pt')\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_predictions = []\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(dataset, desc=\"Testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                if y_true is None:\n",
    "                    y_true = labels.squeeze().cpu().numpy()\n",
    "                    y_pred = (outputs.squeeze().cpu().numpy() > thres).astype(int)\n",
    "                else:\n",
    "                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n",
    "                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n",
    "                \n",
    "                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        metrics = self.evaluate_torchmetrics(y_pred, y_true)\n",
    "        metrics.loc[len(metrics)] = ['loss', running_loss / len(dataset.dataset)]  \n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_torchmetrics(self, y_pred, y_true):\n",
    "        metrics = {\n",
    "            \"accuracy\": BinaryAccuracy(),\n",
    "            \"bal_accuracy\": Accuracy(num_classes=2, task=\"multiclass\", average=\"macro\"),\n",
    "            \"precision\": BinaryPrecision(),\n",
    "            \"recall\": BinaryRecall(),\n",
    "            \"f1_score\": BinaryF1Score(),\n",
    "            \"jaccard_index\": BinaryJaccardIndex(),\n",
    "            \"cohen_kappa\": BinaryCohenKappa(),\n",
    "            \"mcc\": BinaryMatthewsCorrCoef()\n",
    "        }\n",
    "    \n",
    "        y_pred = torch.tensor(y_pred).float()\n",
    "        y_true = torch.tensor(y_true).float()\n",
    "    \n",
    "        # result dataframe\n",
    "        results = pd.DataFrame(columns=[\"Metric\", \"Value\"])\n",
    "        for metric_name, metric in metrics.items():\n",
    "            metric_value = metric(y_pred, y_true)\n",
    "            results.loc[len(results)] = [metric_name, metric_value.item()]  \n",
    "            \n",
    "        return results\n",
    "\n",
    "    def test_visualize(self, n_samples=5, thres=0.5): \n",
    "        self.model.eval()\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                if y_true is None:\n",
    "                    y_true = labels.squeeze().cpu().numpy()\n",
    "                    y_pred = (outputs.squeeze().cpu().numpy() >thres).astype(int)\n",
    "                else:\n",
    "                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n",
    "                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n",
    "                \n",
    "        for _ in range(n_samples):\n",
    "            idx = random.randint(0, len(y_true))\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            axes[0].imshow(y_true[idx].reshape(256, 256), cmap='gray')\n",
    "            axes[0].set_title('True')\n",
    "            axes[1].imshow(y_pred[idx].reshape(256, 256), cmap='gray')\n",
    "            axes[1].set_title('Predicted')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:22.735184Z",
     "iopub.status.busy": "2024-12-16T06:22:22.734723Z",
     "iopub.status.idle": "2024-12-16T06:22:22.739136Z",
     "shell.execute_reply": "2024-12-16T06:22:22.738268Z",
     "shell.execute_reply.started": "2024-12-16T06:22:22.735157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "channels = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:22.740681Z",
     "iopub.status.busy": "2024-12-16T06:22:22.740344Z",
     "iopub.status.idle": "2024-12-16T06:22:31.277787Z",
     "shell.execute_reply": "2024-12-16T06:22:31.276527Z",
     "shell.execute_reply.started": "2024-12-16T06:22:22.740646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:31.280009Z",
     "iopub.status.busy": "2024-12-16T06:22:31.279582Z",
     "iopub.status.idle": "2024-12-16T06:22:32.519958Z",
     "shell.execute_reply": "2024-12-16T06:22:32.519016Z",
     "shell.execute_reply.started": "2024-12-16T06:22:31.279963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_num = head_num\n",
    "        self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
    "\n",
    "        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
    "        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv_layer(x)\n",
    "\n",
    "        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n",
    "        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
    "\n",
    "        x = rearrange(x, \"b h t d -> b t (h d)\")\n",
    "        x = self.out_attention(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp_layers(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
    "        self.mlp = MLP(embedding_dim, mlp_dim)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _x = self.multi_head_attention(x)\n",
    "        _x = self.dropout(_x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        _x = self.mlp(x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_blocks = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_block in self.layer_blocks:\n",
    "            x = layer_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n",
    "                 block_num, patch_dim, classification=True, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_dim = patch_dim\n",
    "        self.classification = classification\n",
    "        self.num_tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "\n",
    "        self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
    "        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
    "\n",
    "        if self.classification:\n",
    "            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_patches = rearrange(x,\n",
    "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n",
    "\n",
    "        batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "        project = self.projection(img_patches)\n",
    "        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "        patches = torch.cat([token, project], dim=1)\n",
    "        patches += self.embedding[:tokens + 1, :]\n",
    "\n",
    "        x = self.dropout(patches)\n",
    "        x = self.transformer(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vit = ViT(img_dim=128,\n",
    "              in_channels=3,\n",
    "              patch_dim=16,\n",
    "              embedding_dim=512,\n",
    "              block_num=6,\n",
    "              head_num=4,\n",
    "              mlp_dim=1024)\n",
    "    print(sum(p.numel() for p in vit.parameters()))\n",
    "    print(vit(torch.rand(1, 3, 128, 128)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:32.522118Z",
     "iopub.status.busy": "2024-12-16T06:22:32.521504Z",
     "iopub.status.idle": "2024-12-16T06:22:34.108725Z",
     "shell.execute_reply": "2024-12-16T06:22:34.107815Z",
     "shell.execute_reply.started": "2024-12-16T06:22:32.522078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class EncoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        width = int(out_channels * (base_width / 64))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_down = self.downsample(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = x + x_down\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_concat=None):\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if x_concat is not None:\n",
    "            x = torch.cat([x_concat, x], dim=1)\n",
    "\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
    "        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
    "        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
    "\n",
    "        self.vit_img_dim = img_dim // patch_dim\n",
    "        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n",
    "                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x1 = self.relu(x)\n",
    "\n",
    "        x2 = self.encoder1(x1)\n",
    "        x3 = self.encoder2(x2)\n",
    "        x = self.encoder3(x3)\n",
    "\n",
    "        x = self.vit(x)\n",
    "        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, x1, x2, x3\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
    "        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
    "        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
    "        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, x1, x2, x3):\n",
    "        x = self.decoder1(x, x3)\n",
    "        x = self.decoder2(x, x2)\n",
    "        x = self.decoder3(x, x1)\n",
    "        x = self.decoder4(x)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(img_dim, in_channels, out_channels,\n",
    "                               head_num, mlp_dim, block_num, patch_dim)\n",
    "\n",
    "        self.decoder = Decoder(out_channels, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x1, x2, x3 = self.encoder(x)\n",
    "        x = self.decoder(x, x1, x2, x3)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "transunet = TransUNet(img_dim=256,\n",
    "                      in_channels=channels,\n",
    "                      out_channels=128,\n",
    "                      head_num=4,\n",
    "                      mlp_dim=512,\n",
    "                      block_num=8,\n",
    "                      patch_dim=16,\n",
    "                      class_num=1).to(device)\n",
    "\n",
    "print(sum(p.numel() for p in transunet.parameters()))\n",
    "print(transunet(torch.randn(1, channels, 256, 256).to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:34.110542Z",
     "iopub.status.busy": "2024-12-16T06:22:34.110294Z",
     "iopub.status.idle": "2024-12-16T06:22:34.116846Z",
     "shell.execute_reply": "2024-12-16T06:22:34.115735Z",
     "shell.execute_reply.started": "2024-12-16T06:22:34.110518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = transunet\n",
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:34.119647Z",
     "iopub.status.busy": "2024-12-16T06:22:34.119380Z",
     "iopub.status.idle": "2024-12-16T06:22:34.162927Z",
     "shell.execute_reply": "2024-12-16T06:22:34.161999Z",
     "shell.execute_reply.started": "2024-12-16T06:22:34.119621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.6, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred, target = pred.squeeze(), target.squeeze()\n",
    "        loss = self.bce_loss(pred, target)\n",
    "        prob = torch.sigmoid(pred)  # Predicted probability\n",
    "        alpha = torch.where(target == 1, self.alpha, 1 - self.alpha)  # Class balancing factor\n",
    "        focal_weight = torch.where(target == 1, 1 - prob, prob)  # Focusing weight\n",
    "        focal_weight = alpha * focal_weight**self.gamma  # Apply alpha and gamma\n",
    "        focal_loss = focal_weight * loss\n",
    "        \n",
    "        focal_loss = focal_loss.sum(dim=(-2,-1)) #*mask\n",
    "        return focal_loss.mean()\n",
    "\n",
    "def loss(pred, target):\n",
    "#     print(pred.shape, target.shape)\n",
    "    bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\") \n",
    "    ll = bce_loss(pred, target)\n",
    "\n",
    "    ll = ll.sum(dim=(-2,-1)) #*mask\n",
    "    return ll.mean()\n",
    "\n",
    "\n",
    "pred = torch.randn(16, 128, 128).float()\n",
    "target = torch.randint(0, 2, (16, 128, 128)).float()\n",
    "\n",
    "# focal_loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "# focal_loss(pred, target)\n",
    "loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:22:34.164313Z",
     "iopub.status.busy": "2024-12-16T06:22:34.164035Z",
     "iopub.status.idle": "2024-12-16T06:49:29.710370Z",
     "shell.execute_reply": "2024-12-16T06:49:29.708839Z",
     "shell.execute_reply.started": "2024-12-16T06:22:34.164284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device, scheduler)\n",
    "trainer.train(num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T06:49:29.714349Z",
     "iopub.status.busy": "2024-12-16T06:49:29.713384Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics_df = trainer.test(test_loader, thres=0.5)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.test_visualize(10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm -r /kaggle/working/*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5968422,
     "sourceId": 9748880,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
