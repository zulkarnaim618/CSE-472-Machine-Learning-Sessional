{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:01.943375Z",
     "iopub.status.busy": "2024-12-16T00:25:01.943019Z",
     "iopub.status.idle": "2024-12-16T00:25:05.112233Z",
     "shell.execute_reply": "2024-12-16T00:25:05.111573Z",
     "shell.execute_reply.started": "2024-12-16T00:25:01.943344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import tifffile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:05.114301Z",
     "iopub.status.busy": "2024-12-16T00:25:05.113943Z",
     "iopub.status.idle": "2024-12-16T00:25:05.122460Z",
     "shell.execute_reply": "2024-12-16T00:25:05.121796Z",
     "shell.execute_reply.started": "2024-12-16T00:25:05.114274Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_RANDOM_SEED = 2021\n",
    "\n",
    "seed = DEFAULT_RANDOM_SEED\n",
    "\n",
    "random.seed(seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:05.124010Z",
     "iopub.status.busy": "2024-12-16T00:25:05.123690Z",
     "iopub.status.idle": "2024-12-16T00:25:05.134864Z",
     "shell.execute_reply": "2024-12-16T00:25:05.134088Z",
     "shell.execute_reply.started": "2024-12-16T00:25:05.123974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SWEDDataset(Dataset):\n",
    "    def __init__(self, root_dir, mode='train', transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.data_dir = os.path.join(root_dir, mode)\n",
    "        self.image_dir = os.path.join(self.data_dir, 'images')\n",
    "        self.label_dir = os.path.join(self.data_dir, 'labels')\n",
    "        \n",
    "        image_files = sorted([f for f in os.listdir(self.image_dir) \n",
    "                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n",
    "        label_files = sorted([f for f in os.listdir(self.label_dir) \n",
    "                            if f.endswith('.npy' if mode in ['train', 'val'] else '.tif')])\n",
    "        \n",
    "        self.pairs = []\n",
    "        label_suffix = '_chip_' if mode in ['train', 'val'] else '_label_'\n",
    "        image_dict = {f.replace('_image_', label_suffix): f for f in image_files}\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            if label_file in image_dict:\n",
    "                self.pairs.append((image_dict[label_file], label_file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file, label_file = self.pairs[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        \n",
    "        if self.mode in ['train', 'val']:\n",
    "            image = np.load(img_path)\n",
    "            label = np.load(label_path)\n",
    "        else:\n",
    "            image = tifffile.imread(img_path)\n",
    "            label = tifffile.imread(label_path)\n",
    "            \n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.mode in ['train', 'val']:\n",
    "            image = image.permute(2, 0, 1)\n",
    "        elif self.mode == 'test':\n",
    "            image = image.permute(0, 2, 1)\n",
    "            label = label.unsqueeze(0)\n",
    "            label = torch.rot90(label, 1, [1, 2])\n",
    "            label = torch.flip(label, [1])\n",
    "\n",
    "        image = image / 2.0**15     # jp2 images are 8 to 16 bit\n",
    "        label = label > 0.0         # binary label\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:05.136721Z",
     "iopub.status.busy": "2024-12-16T00:25:05.136024Z",
     "iopub.status.idle": "2024-12-16T00:25:05.151669Z",
     "shell.execute_reply": "2024-12-16T00:25:05.150960Z",
     "shell.execute_reply.started": "2024-12-16T00:25:05.136694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(root_dir, batch_size=32, num_workers=4, train_transform=None, test_transform=None, device='cuda'):\n",
    "    train_dataset = SWEDDataset(root_dir, mode='train', transform=train_transform)\n",
    "    test_dataset = SWEDDataset(root_dir, mode='test', transform=test_transform)\n",
    "\n",
    "    # train_dataset, val_dataset, _ = random_split(train_dataset, [int(0.01 * len(train_dataset)), \n",
    "    #                                                              int(0.01 * len(train_dataset)), \n",
    "    #                                                              len(train_dataset) - int(0.02 * len(train_dataset))])\n",
    "\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [int(0.8 * len(train_dataset)),  \n",
    "                                                                 len(train_dataset) - int(0.8 * len(train_dataset))])\n",
    "\n",
    "    print(f'Train size: {len(train_dataset)}')\n",
    "    print(f'Validation size: {len(val_dataset)}')\n",
    "    print(f'Test size: {len(test_dataset)}')\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:05.153722Z",
     "iopub.status.busy": "2024-12-16T00:25:05.153454Z",
     "iopub.status.idle": "2024-12-16T00:25:05.166994Z",
     "shell.execute_reply": "2024-12-16T00:25:05.166330Z",
     "shell.execute_reply.started": "2024-12-16T00:25:05.153680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_samples(dataloader, num_samples=5):\n",
    "    # Get a batch\n",
    "    images, masks = next(iter(dataloader))\n",
    "\n",
    "    # Move to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "\n",
    "    # Only display up to the requested number of samples\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
    "    \n",
    "    for idx in range(num_samples):\n",
    "        # Display RGB channels (assuming bands 3,2,1 are RGB)\n",
    "        rgb_img = images[idx][[3,2,1]].permute(1,2,0)\n",
    "        # Normalize for visualization\n",
    "        rgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n",
    "        \n",
    "        axes[0, idx].imshow(rgb_img)\n",
    "        axes[0, idx].axis('off')\n",
    "        axes[0, idx].set_title(f'Image {idx+1}')\n",
    "        \n",
    "        axes[1, idx].imshow(masks[idx][0], cmap='gray')\n",
    "        axes[1, idx].axis('off')\n",
    "        axes[1, idx].set_title(f'Mask {idx+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:05.168188Z",
     "iopub.status.busy": "2024-12-16T00:25:05.167939Z",
     "iopub.status.idle": "2024-12-16T00:25:12.216537Z",
     "shell.execute_reply": "2024-12-16T00:25:12.215500Z",
     "shell.execute_reply.started": "2024-12-16T00:25:05.168165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "root_dir = \"/kaggle/input/sentinel-2-water-edges-dataset/SWED\"\n",
    "train_loader, val_loader, test_loader = get_dataloaders(root_dir, batch_size=32, num_workers=4)\n",
    "\n",
    "# Display 5 samples from training set\n",
    "display_samples(train_loader, num_samples=5)\n",
    "# display_samples(test_loader, num_samples=5)\n",
    "\n",
    "\n",
    "'''\n",
    "# in case we need standardization\n",
    "\n",
    "channel-wise mean:  tensor([ 532.5187,  636.4246,  892.5240, 1049.9366, 1307.1577, 1738.9155,\n",
    "        1915.7476, 1995.0083, 2055.7939, 2086.2705, 2001.6875, 1491.3577])\n",
    "channel-wise std:  tensor([ 679.3956,  750.0253,  923.6580, 1273.5732, 1366.0400, 1500.5621,\n",
    "        1623.3806, 1687.1169, 1720.2144, 1827.5625, 1932.8875, 1631.7715])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:12.217880Z",
     "iopub.status.busy": "2024-12-16T00:25:12.217612Z",
     "iopub.status.idle": "2024-12-16T00:25:32.365058Z",
     "shell.execute_reply": "2024-12-16T00:25:32.363964Z",
     "shell.execute_reply.started": "2024-12-16T00:25:12.217854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade torchmetrics\n",
    "!pip install --upgrade timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:32.367168Z",
     "iopub.status.busy": "2024-12-16T00:25:32.366863Z",
     "iopub.status.idle": "2024-12-16T00:25:34.863047Z",
     "shell.execute_reply": "2024-12-16T00:25:34.862372Z",
     "shell.execute_reply.started": "2024-12-16T00:25:32.367140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    cohen_kappa_score, f1_score, jaccard_score, matthews_corrcoef\n",
    ")\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryJaccardIndex, \n",
    "    BinaryCohenKappa, BinaryMatthewsCorrCoef, Accuracy\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, train_loader, val_loader, test_loader, device, \n",
    "                 scheduler=None, early_stopping_patience=10, min_delta=0.001):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.predictions = None\n",
    "\n",
    "        # Early stopping parameters\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "    def save_checkpoint(self, epoch, train_loss, val_loss, best_model=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses\n",
    "        }\n",
    "        if self.scheduler:\n",
    "            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()\n",
    "\n",
    "        if best_model:\n",
    "            save_path = 'best_model.pt'\n",
    "        else:\n",
    "            save_path = 'checkpoint.pt'\n",
    "        torch.save(checkpoint, save_path)\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_path=\"best_model.pt\"):\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            return 0  # Start from scratch if no checkpoint exists\n",
    "            \n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if self.scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        \n",
    "        # Restore loss history\n",
    "        self.train_losses = checkpoint.get('train_losses', [])\n",
    "        self.val_losses = checkpoint.get('val_losses', [])\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(self.train_loader, desc=\"train\"):\n",
    "            images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        return running_loss / len(self.train_loader.dataset)\n",
    "\n",
    "    def val_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_predictions = []\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"validation\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "                if y_true is None:\n",
    "                    y_true = labels.squeeze().cpu().numpy()\n",
    "                    y_pred = (outputs.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "                else:\n",
    "                    # print(y_true.shape,labels.squeeze().cpu().numpy().shape)\n",
    "                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n",
    "                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n",
    "                \n",
    "                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        metrics = self.evaluate_torchmetrics(y_pred, y_true)\n",
    "        metrics.loc[len(metrics)] = ['loss', running_loss / len(self.val_loader.dataset)]\n",
    "        \n",
    "        return running_loss / len(self.val_loader.dataset), metrics\n",
    "    \n",
    "    def plot_losses(self, train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def train(self, num_epochs, do_plot=True, plot_interval=2, resume=False):\n",
    "        # Initialize or restore from checkpoint\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        start_epoch = self.load_checkpoint('checkpoint.pt') if resume else 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.early_stopped = False\n",
    "\n",
    "        if start_epoch > 0:\n",
    "            print(\"Training resumed from epoch \", start_epoch)\n",
    "    \n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            if self.early_stopped:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_metrics = self.val_epoch()\n",
    "    \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step(val_loss)\n",
    "    \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Training Loss: {train_loss:.4f}\")\n",
    "            print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n",
    "            self.save_checkpoint(epoch + 1, train_loss, val_loss)\n",
    "    \n",
    "            # Early stopping logic\n",
    "            if val_loss < self.best_val_loss - self.min_delta:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.early_stopping_counter = 0\n",
    "                # Save the best model\n",
    "                self.save_checkpoint(epoch + 1, train_loss, val_loss, best_model=True)\n",
    "                print(f\"New best model saved at epoch {epoch + 1}\")\n",
    "            else:\n",
    "                self.early_stopping_counter += 1\n",
    "                print(f\"No improvement. Early stopping counter: {self.early_stopping_counter}\")\n",
    "                \n",
    "                if self.early_stopping_counter >= self.early_stopping_patience:\n",
    "                    self.early_stopped = True\n",
    "                    print(\"Early stopping triggered.\")\n",
    "    \n",
    "            if do_plot and (epoch % plot_interval == 0 or epoch == num_epochs - 1):\n",
    "                self.plot_losses(self.train_losses, self.val_losses)\n",
    "        \n",
    "        # Load the best model at the end of training\n",
    "        if os.path.exists('best_model.pt'):\n",
    "            self.load_checkpoint('best_model.pt')\n",
    "        \n",
    "        return self.early_stopped\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        self.load_checkpoint('best_model.pt')\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_predictions = []\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                if y_true is None:\n",
    "                    y_true = labels.squeeze().cpu().numpy()\n",
    "                    y_pred = (outputs.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "                else:\n",
    "                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n",
    "                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n",
    "                \n",
    "                loss = self.criterion(outputs.squeeze(), labels.squeeze())\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                all_predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        metrics = self.evaluate_torchmetrics(y_pred, y_true)\n",
    "        metrics.loc[len(metrics)] = ['loss', running_loss / len(self.test_loader.dataset)]  \n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_torchmetrics(self, y_pred, y_true):\n",
    "        metrics = {\n",
    "            \"accuracy\": BinaryAccuracy(),\n",
    "            \"bal_accuracy\": Accuracy(num_classes=2, task=\"multiclass\", average=\"macro\"),\n",
    "            \"precision\": BinaryPrecision(),\n",
    "            \"recall\": BinaryRecall(),\n",
    "            \"f1_score\": BinaryF1Score(),\n",
    "            \"jaccard_index\": BinaryJaccardIndex(),\n",
    "            \"cohen_kappa\": BinaryCohenKappa(),\n",
    "            \"mcc\": BinaryMatthewsCorrCoef()\n",
    "        }\n",
    "    \n",
    "        y_pred = torch.tensor(y_pred).float()\n",
    "        y_true = torch.tensor(y_true).float()\n",
    "    \n",
    "        # result dataframe\n",
    "        results = pd.DataFrame(columns=[\"Metric\", \"Value\"])\n",
    "        for metric_name, metric in metrics.items():\n",
    "            metric_value = metric(y_pred, y_true)\n",
    "            results.loc[len(results)] = [metric_name, metric_value.item()]  \n",
    "            \n",
    "        return results\n",
    "\n",
    "    def test_visualize(self, n_samples=5): \n",
    "        self.model.eval()\n",
    "        y_true = None\n",
    "        y_pred = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.test_loader, desc=\"Testing\"):\n",
    "                images, labels = images.to(self.device), labels.to(self.device).float()\n",
    "                outputs = self.model(images)\n",
    "\n",
    "                if y_true is None:\n",
    "                    y_true = labels.squeeze().cpu().numpy()\n",
    "                    y_pred = (outputs.squeeze().cpu().numpy() > 0.5).astype(int)\n",
    "                else:\n",
    "                    y_true = np.concatenate((y_true, labels.squeeze().cpu().numpy()))\n",
    "                    y_pred = np.concatenate((y_pred, (outputs.squeeze().cpu().numpy() > 0.5).astype(int)))\n",
    "                \n",
    "        for _ in range(n_samples):\n",
    "            idx = random.randint(0, len(y_true))\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            axes[0].imshow(y_true[idx].reshape(256, 256), cmap='gray')\n",
    "            axes[0].set_title('True')\n",
    "            axes[1].imshow(y_pred[idx].reshape(256, 256), cmap='gray')\n",
    "            axes[1].set_title('Predicted')\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:34.864613Z",
     "iopub.status.busy": "2024-12-16T00:25:34.864071Z",
     "iopub.status.idle": "2024-12-16T00:25:34.868939Z",
     "shell.execute_reply": "2024-12-16T00:25:34.867998Z",
     "shell.execute_reply.started": "2024-12-16T00:25:34.864573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # import torch.nn as nn\n",
    "# from torchvision import models\n",
    "\n",
    "# class DeepLabV3(nn.Module):\n",
    "#     def __init__(self, num_classes=1):\n",
    "#         super(DeepLabV3, self).__init__()\n",
    "#         # Load pretrained DeepLabV3 model with ResNet50 backbone\n",
    "#         self.model = models.segmentation.deeplabv3_resnet50(pretrained=False)\n",
    "        \n",
    "#         # Modify the first layer to accept 12 channels instead of 3\n",
    "#         self.model.backbone.conv1 = nn.Conv2d(\n",
    "#             in_channels=12,  # Change input channels to 12\n",
    "#             out_channels=self.model.backbone.conv1.out_channels,\n",
    "#             kernel_size=self.model.backbone.conv1.kernel_size,\n",
    "#             stride=self.model.backbone.conv1.stride,\n",
    "#             padding=self.model.backbone.conv1.padding,\n",
    "#             bias=self.model.backbone.conv1.bias is not None\n",
    "#         )\n",
    "        \n",
    "#         # Modify the classifier to output the desired number of classes\n",
    "#         self.model.classifier[4] = nn.Conv2d(\n",
    "#             in_channels=256,\n",
    "#             out_channels=num_classes,\n",
    "#             kernel_size=(1, 1),\n",
    "#             stride=(1, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)['out']\n",
    "\n",
    "# model = DeepLabV3(num_classes=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:34.870528Z",
     "iopub.status.busy": "2024-12-16T00:25:34.870174Z",
     "iopub.status.idle": "2024-12-16T00:25:34.883887Z",
     "shell.execute_reply": "2024-12-16T00:25:34.883059Z",
     "shell.execute_reply.started": "2024-12-16T00:25:34.870503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# def Conv2dSame(in_channels, out_channels, kernel_size, use_bias=True, padding_layer=torch.nn.ReflectionPad2d):\n",
    "#     ka = kernel_size // 2\n",
    "#     kb = ka - 1 if kernel_size % 2 == 0 else ka\n",
    "#     return [\n",
    "#         padding_layer((ka, kb, ka, kb)),\n",
    "#         torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=use_bias)\n",
    "#     ]\n",
    "\n",
    "\n",
    "# def conv2d_bn(in_channels, filters, kernel_size, padding='same', activation='relu'):\n",
    "#     assert padding == 'same'\n",
    "#     affine = False if activation == 'relu' or activation == 'sigmoid' else True\n",
    "#     sequence = []\n",
    "#     sequence += Conv2dSame(in_channels, filters, kernel_size, use_bias=False)\n",
    "#     sequence += [torch.nn.BatchNorm2d(filters, affine=affine)]\n",
    "#     if activation == \"relu\":\n",
    "#         sequence += [torch.nn.ReLU()]\n",
    "#     elif activation == \"sigmoid\":\n",
    "#         sequence += [torch.nn.Sigmoid()]\n",
    "#     elif activation == 'tanh':\n",
    "#         sequence += [torch.nn.Tanh()]\n",
    "#     return torch.nn.Sequential(*sequence)\n",
    "\n",
    "\n",
    "# class MultiResBlock(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, u, alpha=1.67, use_dropout=False):\n",
    "#         super().__init__()\n",
    "#         w = alpha * u\n",
    "#         self.out_channel = int(w * 0.167) + int(w * 0.333) + int(w * 0.5)\n",
    "#         self.conv2d_bn = conv2d_bn(in_channels, self.out_channel, 1, activation=None)\n",
    "#         self.conv3x3 = conv2d_bn(in_channels, int(w * 0.167), 3, activation='relu')\n",
    "#         self.conv5x5 = conv2d_bn(int(w * 0.167), int(w * 0.333), 3, activation='relu')\n",
    "#         self.conv7x7 = conv2d_bn(int(w * 0.333), int(w * 0.5), 3, activation='relu')\n",
    "#         self.bn_1 = torch.nn.BatchNorm2d(self.out_channel)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.bn_2 = torch.nn.BatchNorm2d(self.out_channel)\n",
    "#         self.use_dropout = use_dropout\n",
    "#         if use_dropout:\n",
    "#             self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         if self.use_dropout:\n",
    "#             x = self.dropout(inp)\n",
    "#         else:\n",
    "#             x = inp\n",
    "#         shortcut = self.conv2d_bn(x)\n",
    "#         conv3x3 = self.conv3x3(x)\n",
    "#         conv5x5 = self.conv5x5(conv3x3)\n",
    "#         conv7x7 = self.conv7x7(conv5x5)\n",
    "#         out = torch.cat([conv3x3, conv5x5, conv7x7], dim=1)\n",
    "#         out = self.bn_1(out)\n",
    "#         out = torch.add(shortcut, out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.bn_2(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class ResPathBlock(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, filters):\n",
    "#         super(ResPathBlock, self).__init__()\n",
    "#         self.conv2d_bn1 = conv2d_bn(in_channels, filters, 1, activation=None)\n",
    "#         self.conv2d_bn2 = conv2d_bn(in_channels, filters, 3, activation='relu')\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "#         self.bn = torch.nn.BatchNorm2d(filters)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         shortcut = self.conv2d_bn1(inp)\n",
    "#         out = self.conv2d_bn2(inp)\n",
    "#         out = torch.add(shortcut, out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.bn(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class ResPath(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, filters, length):\n",
    "#         super(ResPath, self).__init__()\n",
    "#         self.first_block = ResPathBlock(in_channels, filters)\n",
    "#         self.blocks = torch.nn.Sequential(*[ResPathBlock(filters, filters) for i in range(length - 1)])\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         out = self.first_block(inp)\n",
    "#         out = self.blocks(out)\n",
    "#         return out\n",
    "\n",
    "\n",
    "# class MultiResUnet(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, nf=32, use_dropout=False):\n",
    "#         super(MultiResUnet, self).__init__()\n",
    "#         self.mres_block1 = MultiResBlock(in_channels, u=nf)\n",
    "#         self.pool = torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         self.res_path1 = ResPath(self.mres_block1.out_channel, nf, 4)\n",
    "\n",
    "#         self.mres_block2 = MultiResBlock(self.mres_block1.out_channel, u=nf * 2)\n",
    "#         # self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         self.res_path2 = ResPath(self.mres_block2.out_channel, nf * 2, 3)\n",
    "\n",
    "#         self.mres_block3 = MultiResBlock(self.mres_block2.out_channel, u=nf * 4)\n",
    "#         # self.pool3 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         self.res_path3 = ResPath(self.mres_block3.out_channel, nf * 4, 2)\n",
    "\n",
    "#         self.mres_block4 = MultiResBlock(self.mres_block3.out_channel, u=nf * 8)\n",
    "#         # self.pool4 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "#         self.res_path4 = ResPath(self.mres_block4.out_channel, nf * 8, 1)\n",
    "\n",
    "#         self.mres_block5 = MultiResBlock(self.mres_block4.out_channel, u=nf * 16)\n",
    "\n",
    "#         self.deconv1 = torch.nn.ConvTranspose2d(self.mres_block5.out_channel, nf * 8, (2, 2), (2, 2))\n",
    "#         self.mres_block6 = MultiResBlock(nf * 8 + nf * 8, u=nf * 8, use_dropout=use_dropout)\n",
    "#         # MultiResBlock(nf * 8 + self.mres_block4.out_channel, u=nf * 8)\n",
    "\n",
    "#         self.deconv2 = torch.nn.ConvTranspose2d(self.mres_block6.out_channel, nf * 4, (2, 2), (2, 2))\n",
    "#         self.mres_block7 = MultiResBlock(nf * 4 + nf * 4, u=nf * 4, use_dropout=use_dropout)\n",
    "#         # MultiResBlock(nf * 4 + self.mres_block3.out_channel, u=nf * 4)\n",
    "\n",
    "#         self.deconv3 = torch.nn.ConvTranspose2d(self.mres_block7.out_channel, nf * 2, (2, 2), (2, 2))\n",
    "#         self.mres_block8 = MultiResBlock(nf * 2 + nf * 2, u=nf * 2, use_dropout=use_dropout)\n",
    "#         # MultiResBlock(nf * 2 + self.mres_block2.out_channel, u=nf * 2)\n",
    "\n",
    "#         self.deconv4 = torch.nn.ConvTranspose2d(self.mres_block8.out_channel, nf, (2, 2), (2, 2))\n",
    "#         self.mres_block9 = MultiResBlock(nf + nf, u=nf)\n",
    "#         # MultiResBlock(nf + self.mres_block1.out_channel, u=nf)\n",
    "\n",
    "#         self.conv10 = conv2d_bn(self.mres_block9.out_channel, out_channels, 1, padding='same', activation='tanh')\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         mresblock1 = self.mres_block1(inp)\n",
    "#         pool = self.pool(mresblock1)\n",
    "#         mresblock1 = self.res_path1(mresblock1)\n",
    "\n",
    "#         mresblock2 = self.mres_block2(pool)\n",
    "#         pool = self.pool(mresblock2)\n",
    "#         mresblock2 = self.res_path2(mresblock2)\n",
    "\n",
    "#         mresblock3 = self.mres_block3(pool)\n",
    "#         pool = self.pool(mresblock3)\n",
    "#         mresblock3 = self.res_path3(mresblock3)\n",
    "\n",
    "#         mresblock4 = self.mres_block4(pool)\n",
    "#         pool = self.pool(mresblock4)\n",
    "#         mresblock4 = self.res_path4(mresblock4)\n",
    "\n",
    "#         mresblock = self.mres_block5(pool)\n",
    "\n",
    "#         up = torch.cat([self.deconv1(mresblock), mresblock4], dim=1)\n",
    "#         mresblock = self.mres_block6(up)\n",
    "\n",
    "#         up = torch.cat([self.deconv2(mresblock), mresblock3], dim=1)\n",
    "#         mresblock = self.mres_block7(up)\n",
    "\n",
    "#         up = torch.cat([self.deconv3(mresblock), mresblock2], dim=1)\n",
    "#         mresblock = self.mres_block8(up)\n",
    "\n",
    "#         up = torch.cat([self.deconv4(mresblock), mresblock1], dim=1)\n",
    "#         mresblock = self.mres_block9(up)\n",
    "\n",
    "#         conv10 = self.conv10(mresblock)\n",
    "#         return conv10\n",
    "\n",
    "\n",
    "# class MultiResUnetGenerator(torch.nn.Module):\n",
    "#     def __init__(self, input_nc, output_nc, ngf=64, use_dropout=False, gpu_ids=[]):\n",
    "#         super(MultiResUnetGenerator, self).__init__()\n",
    "#         self.gpu_ids = gpu_ids\n",
    "\n",
    "#         self.model = MultiResUnet(input_nc, output_nc, nf=ngf, use_dropout=use_dropout)\n",
    "\n",
    "#     def forward(self, inp):\n",
    "#         if self.gpu_ids and isinstance(inp.data, torch.cuda.FloatTensor):\n",
    "#             return torch.nn.parallel.data_parallel(self.model, inp, self.gpu_ids)\n",
    "#         else:\n",
    "#             return self.model(inp)\n",
    "\n",
    "\n",
    "# def weights_init_uniform_rule(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     # for every Linear layer in a model..\n",
    "#     if classname == 'Conv2d':\n",
    "#         pass\n",
    "#     # print(classname)\n",
    "\n",
    "\n",
    "# # a = ResPath(10, 100,3)\n",
    "# # a.apply(weights_init_uniform_rule)\n",
    "# # a = MultiResUnet(512, 512, 3)\n",
    "# # x = torch.randn(2, 3, 512, 512)\n",
    "# # print(a(x).shape\n",
    "# model = MultiResUnet(in_channels=12, out_channels=1, nf=32)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:34.885092Z",
     "iopub.status.busy": "2024-12-16T00:25:34.884883Z",
     "iopub.status.idle": "2024-12-16T00:25:35.981495Z",
     "shell.execute_reply": "2024-12-16T00:25:35.980547Z",
     "shell.execute_reply.started": "2024-12-16T00:25:34.885071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTBinarySegmenter(nn.Module):\n",
    "    def __init__(self, input_channels=3, img_size=256, patch_size=16):\n",
    "        super(ViTBinarySegmenter, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Learnable embedding for patches\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            input_channels, 768, kernel_size=patch_size, stride=patch_size, bias=False\n",
    "        )\n",
    "\n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, (img_size // patch_size) ** 2, 768)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=768, nhead=4, num_encoder_layers=4, dim_feedforward=3072, dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Decoder layers to upsample and predict the segmentation map\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(768, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, self.patch_size*self.patch_size, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, h, w = x.shape\n",
    "\n",
    "        # Ensure input is compatible with fixed image size\n",
    "        if h != self.img_size or w != self.img_size:\n",
    "            x = F.interpolate(x, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Patch embedding\n",
    "        patches = self.patch_embed(x)  # Shape: (B, 768, H/P, W/P)\n",
    "        patches = patches.flatten(2).transpose(1, 2)  # Shape: (B, N, 768)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        patches = patches + self.pos_embed\n",
    "\n",
    "        # Pass through transformer\n",
    "        transformed = self.transformer(patches, patches)  # Shape: (B, N, 768)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        features = transformed.permute(0, 2, 1).reshape(\n",
    "            batch_size, 768, self.img_size // self.patch_size, self.img_size // self.patch_size\n",
    "        )\n",
    "\n",
    "        # Decode the features into a segmentation map\n",
    "        seg_map = self.decoder(features)\n",
    "\n",
    "        # Resize output to match the input size\n",
    "        # seg_map = F.interpolate(seg_map, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        seg_map = seg_map.reshape(batch_size, h, w)\n",
    "\n",
    "        return torch.sigmoid(seg_map)\n",
    "\n",
    "\n",
    "model = ViTBinarySegmenter(input_channels=12, img_size=256, patch_size=16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:35.983613Z",
     "iopub.status.busy": "2024-12-16T00:25:35.982829Z",
     "iopub.status.idle": "2024-12-16T00:25:35.987719Z",
     "shell.execute_reply": "2024-12-16T00:25:35.986766Z",
     "shell.execute_reply.started": "2024-12-16T00:25:35.983572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = torch.hub.load(\n",
    "#     'mateuszbuda/brain-segmentation-pytorch', \n",
    "#     'unet',\n",
    "#     in_channels=12,  # Set input channels to 12\n",
    "#     out_channels=1,  # Output channel remains 1 for the mask\n",
    "#     init_features=32, \n",
    "#     pretrained=False  # Pretraining is not available for 12 channels\n",
    "# )\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:35.989233Z",
     "iopub.status.busy": "2024-12-16T00:25:35.988781Z",
     "iopub.status.idle": "2024-12-16T00:25:36.036900Z",
     "shell.execute_reply": "2024-12-16T00:25:36.036164Z",
     "shell.execute_reply.started": "2024-12-16T00:25:35.989207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.65, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred, target = pred.squeeze(), target.squeeze()\n",
    "        loss = self.bce_loss(pred, target)\n",
    "        prob = torch.sigmoid(pred)  # Predicted probability\n",
    "        alpha = torch.where(target == 1, self.alpha, 1 - self.alpha)  # Class balancing factor\n",
    "        focal_weight = torch.where(target == 1, 1 - prob, prob)  # Focusing weight\n",
    "        focal_weight = alpha * focal_weight**self.gamma  # Apply alpha and gamma\n",
    "        focal_loss = focal_weight * loss\n",
    "        \n",
    "        focal_loss = focal_loss.sum(dim=(-2,-1)) #*mask\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "pred = torch.randn(16, 128, 128).float()\n",
    "target = torch.randint(0, 2, (16, 128, 128)).float()\n",
    "\n",
    "focal_loss = FocalLoss(alpha=0.75, gamma=2.0)\n",
    "focal_loss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T00:25:36.039862Z",
     "iopub.status.busy": "2024-12-16T00:25:36.039578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, train_loader, val_loader, test_loader, device, scheduler)\n",
    "trainer.train(num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics_df = trainer.test()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.test_visualize(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5968422,
     "sourceId": 9748880,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
